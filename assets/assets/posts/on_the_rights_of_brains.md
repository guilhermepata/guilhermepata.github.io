On The Rights Of Brains—Organic Or Otherwise
===
Why Neuroethics Matters To Artificial Intelligence
---

In 1950, Alan Turing devised a test to determine whether a machine has human-like intelligence, which he dubbed the ‘imitation game’: a human tester is tasked with determining which one of two different ‘players’ is human and which is a computer, using only written responses to written questions. [1] Today, intelligent machines like GPT-3 — which can have text conversations, write articles, and even develop functioning computer code — show that we are getting eerily close to developing machines that can pass the Turing test. [2] While in the short term we may disregard fears of a singularity — the creation of an artificial superintelligent agent — these developments should lead us to reconsider the moral standing of virtual minds. Moreover, they should remind us that when it comes to caring for other minds, humanity’s track record falls short.

![HAL9000](https://miro.medium.com/max/3600/1*hRRLnrJODSlC2Er2AhIqNg.jpeg)

The question of whether or not a machine that passed the Turing test would be able to *think*, or even if it would be *sentient*, is still the subject of heated debate. Turing himself believed that such a distinction was meaningless, on the grounds that we do not have a clear definition for ‘thinking’. The linguist and philosopher Noam Chomsky agrees that the question is ill-posed, seeing it only as a semantic discussion. From his perspective, since humans and computers are inescapably different, a computer can ‘think’ (like a human) insofar as a submarine can ‘swim’ (like a fish), or an airplane can ‘fly’ (like a bird). [3] On the other hand, functionalists like the philosopher of mind Daniel Dennet hold a very different view. If a machine passes the Turing test, then, he would claim, we have no experimental evidence that can distinguish it from a human mind. Therefore, we must conclude that whatever ‘goings-on’ there may be in the machine’s ‘mind’ will be so analogous to human thinking that we have to afford it the capacity for thought. Dennet goes on to defend that any being indistinguishable from a human in *all* regards *must* be sentient. [4] Hence, he denies the possibility of ‘philosophical zombies’ — or, if we are being brazen, he views all humans as such.

But determining a being’s sentience by focusing on its ability to speak is misguided. In fact, the word was originally used to distinguish between the capacity to think — *reason* — and the capacity to feel — *sense*. We might wonder if speaking and thinking are insufficient for sentience, but we seem to be sure that they are not necessary. After all, the overwhelming majority of scientists and philosophers of the modern-day — unlike those of the early enlightenment — ascribe consciousness to all mammals and birds, and some go even further. Here, consciousness is usually used to mean ‘self-awareness’ or ‘perception of the self’, and hence it implies sentience. Contradictorily, neurologists, such as Dick Swaab, have claimed that consciousness requires the cerebral cortex, [5] which would deny the possibility of it in any being very different from a human. But we must be careful with anthropocentric ideas. It is more likely that neurologists mean something different from other thinkers when using the term ‘consciousness’. We can see their usage as being closer to a medical one, in much the same way that physicians use the term ‘alive’ distinctly from molecular biologists. Indeed, The Cambridge Declaration On Consciousness, written by neuroscientists and neurophysiologists, states that the absence of a cerebral cortex does not preclude an animal from being conscious. Accordingly, its authors attribute consciousness to mammals, birds, and “many other creatures, including octopuses”. [6]

What seems to be the scientific consensus notwithstanding, there is still a wide gap between it and the legal status of animals. Many experts and activist groups believe this needs to change. One of these is the Nonhuman Rights Project, which has argued for the legal personhood of great apes, elephants, and cetaceans, and even secured sanctuary for apes and elephants previously held in confinement. [7] However unfortunate, this gap is not merely legal: it reflects the view of the general population, the majority of whom have scant concern for the commodification and mistreatment of (nonhuman) animal lives. People confound sentience with rationality to justify why a farm animal is less deserving of a dignified life than a human. How can cattle have the same moral worth as us, it is argued, when a cow cannot write a poem, paint on a canvas, or even understand its own mortality? This is despite counterarguments having been given centuries ago. In 1780, the philosopher Jeremy Bentham put it best when he wrote ‘the question is not, Can they *reason*? nor, Can they *talk*? but, Can they *suffer*?’. [8] This statement is at the heart of the recognition of (nonhuman) animal rights, and it illustrates why our focus cannot be on the intelligence of a being, but on its sentience.

Nevertheless, since solving the problem of sentience is a tremendous challenge, we are still faced with many uncertain situations — are lobsters sentient, for instance? In a 2015 essay, philosopher Jeff Sebo presented three different principles of action we could consider in such cases. We could be incautionary: treat uncertain beings as non-sentient. Alternatively, we can take a precautionary approach: treat all possibly sentient beings as sentient. Lastly, we can abide by the expected value principle: consider both the possibility of sentience and the moral impact of the action together. Sebo argued that the incautionary principle is immoral and should be disregarded, on the basis that following it risks harming beings to whom we may attribute any small chance of sentience, based solely on our ignorance. He then argued that both the precautionary and expected value principles imply that we must treat other animals, including invertebrates, “much better than we currently do”. [9] In light of such ideas, many thinkers have advocated for the positive rights of not only mammals, birds, and so on, but even insects. [10]

And here is where we must return to the question of machine sentience. Presently, many have claimed that software cannot possibly be sentient, on the basis that it is just “number crunching” on a mechanistic device. If sentience is not needed to explain digital minds, why bring it up? However, some experts believe that we might one day map the human brain so perfectly as to be able to simulate it on a computer, which would betray the flaws in this argument. This development would also pose a serious ethical problem — would deleting a file be comparable to murder? For now, we can rest assured that the computational and labor requirements mean that, if possible, we will only achieve this far into the future. But we may not need to wait long to be faced with an ethical conundrum. The research group OpenWorm has been working with a simulation of the nematode *Caenorhabditis elegans*, whose ‘brain’ is based solely on the connectome of the organism’s nervous system. [11] In 2014, they implemented their simulation in a physical robot. It exhibited life-like behavior, moving towards ‘food’ and evading ‘touch’. [12]

Although this simulation still has its limitations, we have to wonder: is it sentient? Sebo’s precautionary principle says that maybe we should suppose so, and his expected value principle could make us hesitate. If hundreds of billions of virtual nematodes could be simulated in a supercomputer, ‘harming’ them should perhaps be given a little more moral consideration than harming a single real nematode. This issue only worsens as we move to more sophisticated beings. The 302 neurons of *C. elegans* pale in comparison to a human’s 86 billion, but what about when weighed up against a bee’s 1 million? [13, 14] That is nearly five orders of magnitude from the insect to us, but only less than four from the nematode to the insect. How far away are we from simulating a bee, and what would be the implications? If some authors believe we should grant rights to living insects, what about digital ones? Regardless, we must bear in mind that the rights we afford to other beings should fit with their needs and interests — they should not be the rights we grant to humans. Figuring this out is a challenge on its own.

Finally, our concerns ought not to end with animals and simulations thereof. Some philosophers, like David Chalmers, consider the possibility of something even as simple as a thermostat having a ‘subjective experience’. [15] Most find this hypothesis to be too remote, and, be it as it may, not much can be said about the needs and wants of thermostats. But consider, instead, the sophistication of artificial neural networks, like GPT-3 and its kind, which mimic the structure and behavior of natural nervous systems, in many ways other than just written text. If we buy into the functional approach of philosophers like Dennet and contemplate a precautionary or expected value principle of action, we should ask: do they have a subjective experience? And if so, can they *suffer*? Untangling the technical and philosophical mysteries behind this question is not merely of scientific interest — it is an ethical imperative.

# References
[1] Alan M. Turing. *[“I. — Computing Machinery And Intelligence”](https://doi.org/10.1093/mind/lix.236.433)*. In: Mind LIX.236 (Oct. 1950), pp. 433– 460. DOI: 10.1093/mind/lix.236.433.
[2] Tom B. Brown et al. *[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)*. 2020. eprint: arXiv:2005.14165.
[3] Noam Chomsky and Lawrence Krauss. *[An Origins Project Dialogue](https://youtu.be/tbxp8ViBTu8?t=500)*. 2015.
[4] Daniel Dennett. *Consciousness explained*. London: Penguin, 1993. ISBN: 978–0–7139–9037–9.
[5] D. F. Swaab. *We are our brains: a neurobiography of the brain, from the womb to Alzheimer’s*. New York: Spiegel & Grau, 2014. ISBN: 978–0812992960.
[6] Philip Low et al. *[The Cambridge Declaration On Consciousness](http://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf)*. University of Cambridge, 2012.
[7] [Nonhuman Rights Project](https://www.nonhumanrights.org/).
[8] Jeremy Bentham. [“Of Circumstances Influencing Sensibility”](http://www.koeblergerhard.de/Fontes/BenthamJeremyMoralsandLegislation1789.pdf#page=90). In: *An Introduction to the Principles of Morals and Legislation*. London: T. Payne and Sons, 1780.
[9] Jeff Sebo. *[Reconsider The Lobster](https://jeffsebodotnet.files.wordpress.com/2015/07/reconsider-the-lobster.pdf)*. UNC-Chapel Hill, 2015.
[10] Simon Knutsson. [“Reducing Suffering Amongst Invertebrates Such As Insects”](https://was-research.org/writing-by-others/reducingsuffering-amongst-invertebrates-insects/). In: *Wild-Animal Suffering Research* (2016).
[11] Gopal P. Sarma et al. [“OpenWorm: overview and recent advances in integrative biological simulation of Caenorhabditis elegans”](https://doi.org/10.1098/rstb.2017.0382). In: *Philosophical Transactions of the Royal Society B: Biological Sciences* 373.1758 (Sept. 2018), p. 20170382. DOI: 10.1098/rstb.2017.0382.
[12] Fiona MacDonald. [“Scientists Put a Worm Brain in a Lego Robot Body — And It Worked”](https://www.sciencealert.com/scientists-put-worm-brain-in-legorobot-openworm-connectome). In: *Science Altert* (2017).
[13] Frederico A.C. Azevedo et al. “[Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain](https://doi.org/10.1002/cne.21974)”. In: *The Journal of Comparative Neurology* 513.5 (Apr. 2009), pp. 532–541. DOI: 10.1002/cne.21974.
[14] Randolf Menzel and Martin Giurfa. “Cognitive architecture of a mini-brain: the honeybee”. In: *Trends in Cognitive Sciences* 5.2 (Feb. 2001), pp. 62–71. DOI: 10.1016/s1364–6613(00)01601–6.
[15] David Chalmers. [“What Is It Like To Be A Thermostat?”](https://annakaharris.com/chalmers/) In: The conscious mind: in search of a fundamental theory. New York: Oxford University Press, 1996. ISBN: 978–0195117899.